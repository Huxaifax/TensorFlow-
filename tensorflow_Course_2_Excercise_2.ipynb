{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "tensorflow_Course_2_Excercise_2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMy4/lHmM7WFJT0OiJ/tqQ8",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Huxaifax/TensorFlow-/blob/main/tensorflow_Course_2_Excercise_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import os\n",
        "import zipfile\n",
        "import random\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from shutil import copyfile\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "HSolmy3WZZxr"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If the URL doesn't work, visit https://www.microsoft.com/en-us/download/confirmation.aspx?id=54765\n",
        "# And right click on the 'Download Manually' link to get a new URL to the dataset\n",
        "\n",
        "# Note: This is a very large dataset and will take some time to download\n",
        "\n",
        "# Download the dataset\n",
        "!wget https://storage.googleapis.com/tensorflow-1-public/course2/cats_and_dogs_filtered.zip\n",
        "\n",
        "# Extract the archive\n",
        "zip_ref = zipfile.ZipFile(\"./cats_and_dogs_filtered.zip\", 'r')\n",
        "zip_ref.extractall(\"tmp/\")\n",
        "zip_ref.close()\n"
      ],
      "metadata": {
        "id": "iBjxQCclGNWZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bba3ef87-1e24-4bee-f593-87baff77f7b0"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2022-06-12 05:45:07--  https://storage.googleapis.com/tensorflow-1-public/course2/cats_and_dogs_filtered.zip\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 74.125.135.128, 74.125.195.128, 2607:f8b0:400e:c06::80, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|74.125.135.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 68606236 (65M) [application/zip]\n",
            "Saving to: ‘cats_and_dogs_filtered.zip.2’\n",
            "\n",
            "cats_and_dogs_filte 100%[===================>]  65.43M   256MB/s    in 0.3s    \n",
            "\n",
            "2022-06-12 05:45:07 (256 MB/s) - ‘cats_and_dogs_filtered.zip.2’ saved [68606236/68606236]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define root directory\n",
        "root_dir = '/tmp/cats-v-dogs'\n",
        "\n",
        "# Empty directory to prevent FileExistsError is the function is run several times\n",
        "if os.path.exists(root_dir):\n",
        "  shutil.rmtree(root_dir)\n",
        "\n",
        "# GRADED FUNCTION: create_train_test_dirs\n",
        "def create_train_test_dirs(root_path):\n",
        "  ### START CODE HERE\n",
        "\n",
        "  # HINT:\n",
        "  # Use os.makedirs to create your directories with intermediate subdirectories\n",
        "  # Don't hardcode the paths. Use os.path.join to append the new directories to the root_path parameter\n",
        "  try:\n",
        "      os.makedirs(os.path.join(root_dir))\n",
        "      os.makedirs(os.path.join(root_dir, \"training\"))\n",
        "      os.makedirs(os.path.join(root_dir, \"training\", \"cats\"))\n",
        "      os.makedirs(os.path.join(root_dir, \"training\", \"dogs\"))\n",
        "\n",
        "      os.makedirs(os.path.join(root_dir, \"testing\"))\n",
        "      os.makedirs(os.path.join(root_dir, \"testing\", \"cats\"))\n",
        "      os.makedirs(os.path.join(root_dir, \"testing\", \"dogs\"))\n",
        "  except:\n",
        "      pass\n",
        "  \n",
        "  ### END CODE HERE\n",
        "\n",
        "  \n",
        "try:\n",
        "  create_train_test_dirs(root_path=root_dir)\n",
        "except FileExistsError:\n",
        "  print(\"You should not be seeing this since the upper directory is removed beforehand\")"
      ],
      "metadata": {
        "id": "UUeOMCMgBjLR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfa19cda-a3a6-4ce9-f202-c12ffa4cb553"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(506,)"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your create_train_test_dirs function\n",
        "\n",
        "for rootdir, dirs, files in os.walk(root_dir):\n",
        "    for subdir in dirs:\n",
        "        print(os.path.join(rootdir, subdir))"
      ],
      "metadata": {
        "id": "QyHtPGx8Bs8j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: split_data\n",
        "def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n",
        "\n",
        "  ### START CODE HERE\n",
        "    all_files = []\n",
        "    \n",
        "    for file_name in os.listdir(SOURCE):\n",
        "        file_path = SOURCE + file_name\n",
        "\n",
        "        if os.path.getsize(file_path):\n",
        "            all_files.append(file_name)\n",
        "        else:\n",
        "            print('{} is zero length, so ignoring'.format(file_name))\n",
        "    \n",
        "    n_files = len(all_files)\n",
        "    split_point = int(n_files * SPLIT_SIZE)\n",
        "    \n",
        "    shuffled = random.sample(all_files, n_files)\n",
        "    \n",
        "    train_set = shuffled[:split_point]\n",
        "    test_set = shuffled[split_point:]\n",
        "    \n",
        "    for file_name in train_set:\n",
        "        copyfile(SOURCE + file_name, TRAINING + file_name)\n",
        "        \n",
        "    for file_name in test_set:\n",
        "        copyfile(SOURCE + file_name, TESTING + file_name)\n",
        "\n",
        "  ### END CODE HERE"
      ],
      "metadata": {
        "id": "_JsFgBBYWCwM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your split_data function\n",
        "\n",
        "# Define paths\n",
        "CAT_SOURCE_DIR = \"/tmp/PetImages/Cat/\"\n",
        "DOG_SOURCE_DIR = \"/tmp/PetImages/Dog/\"\n",
        "\n",
        "TRAINING_DIR = \"/tmp/cats-v-dogs/training/\"\n",
        "TESTING_DIR = \"/tmp/cats-v-dogs/testing/\"\n",
        "\n",
        "TRAINING_CATS_DIR = os.path.join(TRAINING_DIR, \"cats/\")\n",
        "TESTING_CATS_DIR = os.path.join(TESTING_DIR, \"cats/\")\n",
        "\n",
        "TRAINING_DOGS_DIR = os.path.join(TRAINING_DIR, \"dogs/\")\n",
        "TESTING_DOGS_DIR = os.path.join(TESTING_DIR, \"dogs/\")\n",
        "\n",
        "# Empty directories in case you run this cell multiple times\n",
        "if len(os.listdir(TRAINING_CATS_DIR)) > 0:\n",
        "  for file in os.scandir(TRAINING_CATS_DIR):\n",
        "    os.remove(file.path)\n",
        "if len(os.listdir(TRAINING_DOGS_DIR)) > 0:\n",
        "  for file in os.scandir(TRAINING_DOGS_DIR):\n",
        "    os.remove(file.path)\n",
        "if len(os.listdir(TESTING_CATS_DIR)) > 0:\n",
        "  for file in os.scandir(TESTING_CATS_DIR):\n",
        "    os.remove(file.path)\n",
        "if len(os.listdir(TESTING_DOGS_DIR)) > 0:\n",
        "  for file in os.scandir(TESTING_DOGS_DIR):\n",
        "    os.remove(file.path)\n",
        "\n",
        "# Define proportion of images used for training\n",
        "split_size = .9\n",
        "\n",
        "# Run the function\n",
        "# NOTE: Messages about zero length images should be printed out\n",
        "split_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, TESTING_CATS_DIR, split_size)\n",
        "split_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, TESTING_DOGS_DIR, split_size)\n",
        "\n",
        "# Check that the number of images matches the expected output\n",
        "print(f\"\\n\\nThere are {len(os.listdir(TRAINING_CATS_DIR))} images of cats for training\")\n",
        "print(f\"There are {len(os.listdir(TRAINING_DOGS_DIR))} images of dogs for training\")\n",
        "print(f\"There are {len(os.listdir(TESTING_CATS_DIR))} images of cats for testing\")\n",
        "print(f\"There are {len(os.listdir(TESTING_DOGS_DIR))} images of dogs for testing\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ucXc5rjmWDdz",
        "outputId": "819a6391-ce1a-43b1-a962-0e72a896ced8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "KNeighborsRegressor()"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "TRAINING_DIR = '/tmp/cats-v-dogs/training'\n",
        "VALIDATION_DIR = '/tmp/cats-v-dogs/testing'\n",
        "# GRADED FUNCTION: train_val_generators\n",
        "def train_val_generators(TRAINING_DIR, VALIDATION_DIR):\n",
        "  ### START CODE HERE\n",
        "  # Instantiate the ImageDataGenerator class (don't forget to set the arguments to augment the images)\n",
        "  train_datagen = ImageDataGenerator(rescale=1.0/255,\n",
        "                                     rotation_range=40,\n",
        "                                     width_shift_range=0.2,\n",
        "                                     height_shift_range=0.2,\n",
        "                                     shear_range=0.2,\n",
        "                                     zoom_range=0.2,\n",
        "                                     horizontal_flip=True,\n",
        "                                     fill_mode='nearest')\n",
        "\n",
        "  # Pass in the appropriate arguments to the flow_from_directory method\n",
        "  train_generator = train_datagen.flow_from_directory(directory=TRAINING_DIR,\n",
        "                                                      batch_size=64,\n",
        "                                                      class_mode='binary',\n",
        "                                                      target_size=(150, 150))\n",
        "\n",
        "  # Instantiate the ImageDataGenerator class (don't forget to set the rescale argument)\n",
        "  validation_datagen =  ImageDataGenerator(rescale=1.0/255,\n",
        "                                     rotation_range=40,\n",
        "                                     width_shift_range=0.2,\n",
        "                                     height_shift_range=0.2,\n",
        "                                     shear_range=0.2,\n",
        "                                     zoom_range=0.2,\n",
        "                                     horizontal_flip=True,\n",
        "                                     fill_mode='nearest')\n",
        "\n",
        "  # Pass in the appropriate arguments to the flow_from_directory method\n",
        "  validation_generator = validation_datagen.flow_from_directory(directory=VALIDATION_DIR,\n",
        "                                                                batch_size=64,\n",
        "                                                                class_mode='binary',\n",
        "                                                                target_size=(150, 150))\n",
        "  ### END CODE HERE\n",
        "  return train_generator, validation_generator"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rRSfaJh_9dOb",
        "outputId": "aa50d647-23f6-4213-ce4b-bd6115953a2c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([21.78, 22.9 , 25.36, 26.06, 27.1 , 27.1 , 20.88, 19.1 , 18.4 ,\n",
              "       19.48, 19.28, 22.  , 24.34, 20.52, 24.66, 21.3 , 30.48, 20.4 ,\n",
              "       15.7 , 23.54, 16.82, 17.64, 18.3 , 17.08, 16.66, 15.1 , 16.78,\n",
              "       14.94, 19.94, 18.34, 14.1 , 16.82, 15.12, 14.1 , 15.12, 26.92,\n",
              "       22.14, 27.4 , 28.44, 31.88, 31.88, 25.36, 25.36, 24.22, 20.68,\n",
              "       20.44, 20.44, 18.1 , 18.1 , 24.  , 21.54, 24.  , 27.16, 27.16,\n",
              "       25.7 , 39.82, 27.08, 38.28, 24.8 , 25.64, 21.78, 33.6 , 21.78,\n",
              "       24.06, 31.74, 25.3 , 26.98, 22.18, 20.42, 20.42, 27.76, 29.5 ,\n",
              "       27.76, 27.76, 22.92, 21.64, 25.82, 21.64, 21.38, 22.02, 24.8 ,\n",
              "       21.88, 25.22, 25.64, 25.98, 25.98, 23.28, 25.98, 24.02, 25.58,\n",
              "       25.58, 25.06, 26.34, 26.04, 30.1 , 24.84, 23.62, 24.32, 28.52,\n",
              "       24.96, 22.1 , 22.2 , 15.34, 19.74, 19.74, 19.66, 19.56, 21.34,\n",
              "       19.66, 19.56, 22.08, 20.1 , 19.6 , 17.54, 20.1 , 17.7 , 20.2 ,\n",
              "       20.1 , 20.66, 19.8 , 22.76, 20.6 , 19.66, 18.52, 19.66, 20.6 ,\n",
              "       18.52, 16.62, 18.04, 16.88, 18.4 , 18.4 , 18.78, 18.56, 20.24,\n",
              "       17.44, 17.8 , 18.4 , 15.88, 17.06, 15.24, 14.76, 15.62, 15.62,\n",
              "       15.62, 18.26, 18.26, 15.62, 17.82, 17.44, 37.22, 20.66, 19.28,\n",
              "       20.24, 20.24, 15.34, 15.34, 37.78, 25.52, 32.08, 20.66, 42.56,\n",
              "       44.54, 44.54, 30.34, 20.24, 37.22, 19.52, 19.98, 20.24, 19.98,\n",
              "       18.74, 21.9 , 24.4 , 23.74, 25.82, 22.34, 24.2 , 23.84, 38.56,\n",
              "       33.24, 38.56, 33.24, 31.6 , 33.24, 38.56, 37.84, 32.72, 33.14,\n",
              "       32.72, 33.14, 32.72, 33.72, 31.8 , 31.8 , 34.9 , 26.78, 25.24,\n",
              "       26.78, 29.38, 29.5 , 25.2 , 25.3 , 41.28, 41.28, 23.76, 23.78,\n",
              "       20.74, 22.9 , 21.1 , 21.1 , 21.1 , 23.9 , 28.84, 22.6 , 27.28,\n",
              "       22.96, 21.9 , 21.1 , 23.38, 25.42, 17.32, 31.8 , 24.46, 37.82,\n",
              "       36.46, 36.14, 35.96, 29.5 , 29.44, 34.  , 41.28, 38.28, 38.16,\n",
              "       28.12, 29.3 , 34.12, 34.12, 21.44, 21.92, 21.44, 21.4 , 22.1 ,\n",
              "       21.74, 20.  , 19.68, 22.16, 20.  , 21.38, 31.22, 31.22, 26.28,\n",
              "       29.56, 31.22, 27.08, 24.86, 38.28, 42.44, 38.9 , 36.48, 38.82,\n",
              "       41.88, 41.88, 37.9 , 41.88, 34.6 , 38.82, 36.16, 32.42, 31.74,\n",
              "       32.58, 28.82, 31.74, 26.88, 31.96, 31.96, 31.96, 31.96, 31.96,\n",
              "       30.58, 31.74, 32.58, 36.62, 42.8 , 24.84, 21.88, 38.64, 21.88,\n",
              "       24.44, 22.62, 34.9 , 34.9 , 31.88, 24.54, 23.28, 24.44, 23.22,\n",
              "       22.94, 25.28, 29.12, 25.42, 25.78, 28.02, 30.58, 31.22, 27.02,\n",
              "       31.96, 27.02, 23.72, 21.6 , 29.  , 21.32, 21.02, 20.94, 21.44,\n",
              "       21.6 , 18.54, 19.52, 20.5 , 21.3 , 23.32, 23.76, 23.32, 22.9 ,\n",
              "       22.06, 23.76, 26.14, 22.06, 19.7 , 21.22, 19.92, 21.86, 22.98,\n",
              "       23.6 , 21.16, 20.78, 25.74, 24.3 , 20.72, 22.64, 24.32, 24.44,\n",
              "       19.8 , 29.36, 26.58, 19.  , 18.84, 26.48, 33.32, 25.78, 25.78,\n",
              "       28.  , 30.46, 42.8 , 20.96, 24.8 , 15.88, 19.06, 20.94, 21.42,\n",
              "       33.8 , 25.6 , 30.94, 25.6 , 27.22, 27.22, 16.98, 14.58, 39.16,\n",
              "       39.16, 26.46, 36.6 , 27.22, 11.36, 10.54, 10.82, 12.36, 12.5 ,\n",
              "        9.6 , 10.22,  6.86, 10.26, 11.62, 11.62, 14.16, 11.  ,  8.94,\n",
              "        9.74, 13.18, 12.5 , 13.52, 21.66, 11.88, 15.58, 11.74, 13.54,\n",
              "       13.98, 12.46,  8.5 , 14.82,  9.54, 11.14, 15.88, 10.5 , 14.28,\n",
              "        6.86, 13.18, 18.56, 14.52, 17.62, 10.34, 12.74, 11.88, 17.18,\n",
              "       10.92, 11.88, 11.2 , 14.58, 11.96, 10.44, 16.98, 16.98, 14.16,\n",
              "       13.58, 12.94, 11.74, 12.56, 10.26, 13.52, 11.58, 14.  , 12.6 ,\n",
              "       13.52, 13.3 , 12.9 , 12.16, 12.1 , 10.56, 12.12, 11.96, 10.6 ,\n",
              "       14.56, 13.84, 13.34, 13.44, 12.44, 16.98, 13.34, 14.48, 16.06,\n",
              "       13.58, 15.54, 17.04, 16.62, 12.54, 12.2 , 13.58, 12.94, 14.84,\n",
              "       20.24, 14.34, 19.82, 20.24, 20.38, 22.28, 17.56, 12.74, 18.56,\n",
              "       23.7 , 21.26, 20.24, 18.6 , 22.72, 23.28, 15.54, 16.06, 14.18,\n",
              "       14.96, 15.88, 16.36, 22.28, 22.72, 23.44, 20.86, 22.8 , 21.34,\n",
              "       21.42, 21.34, 17.04, 11.54, 12.28, 14.86, 18.3 , 22.08, 21.82,\n",
              "       22.02, 18.7 , 18.7 , 20.64, 18.7 , 19.96, 21.18, 23.12, 20.88,\n",
              "       21.9 , 21.42])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your generators\n",
        "train_generator, validation_generator = train_val_generators(TRAINING_DIR, TESTING_DIR)"
      ],
      "metadata": {
        "id": "sIrb57uB9hmJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.optimizers import RMSprop\n",
        "# GRADED FUNCTION: create_model\n",
        "def create_model():\n",
        "  # DEFINE A KERAS MODEL TO CLASSIFY CATS V DOGS\n",
        "  # USE AT LEAST 3 CONVOLUTION LAYERS\n",
        "\n",
        "  ### START CODE HERE\n",
        "\n",
        "  model = tf.keras.models.Sequential([\n",
        "    tf.keras.layers.Conv2D(32, (3,3), input_shape = (150, 150, 3), activation = tf.nn.relu),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(64, (3,3), activation = tf.nn.relu),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Conv2D(128, (3, 3), activation = tf.nn.relu),\n",
        "    tf.keras.layers.MaxPooling2D(2,2),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(512, activation = tf.nn.relu),\n",
        "    tf.keras.layers.Dense(128, activation = tf.nn.relu),\n",
        "    tf.keras.layers.Dense(1, activation = tf.nn.sigmoid)\n",
        "])\n",
        "\n",
        "  \n",
        "  model.compile( optimizer = RMSprop(lr=0.001),\n",
        "                loss = 'binary_crossentropy',\n",
        "                metrics = ['accuracy']) \n",
        "    \n",
        "  ### END CODE HERE\n",
        "\n",
        "  return model"
      ],
      "metadata": {
        "id": "0zFbMJI6UcPf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Get the untrained model\n",
        "model = create_model()\n",
        "\n",
        "# Train the model\n",
        "# Note that this may take some time.\n",
        "history = model.fit(train_generator,\n",
        "                    epochs=15,\n",
        "                    verbose=1,\n",
        "                    validation_data=validation_generator)"
      ],
      "metadata": {
        "id": "be_mQJo3Ulib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#-----------------------------------------------------------\n",
        "# Retrieve a list of list results on training and test data\n",
        "# sets for each training epoch\n",
        "#-----------------------------------------------------------\n",
        "acc=history.history['accuracy']\n",
        "val_acc=history.history['val_accuracy']\n",
        "loss=history.history['loss']\n",
        "val_loss=history.history['val_loss']\n",
        "\n",
        "epochs=range(len(acc)) # Get number of epochs\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation accuracy per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n",
        "plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.show()\n",
        "print(\"\")\n",
        "\n",
        "#------------------------------------------------\n",
        "# Plot training and validation loss per epoch\n",
        "#------------------------------------------------\n",
        "plt.plot(epochs, loss, 'r', \"Training Loss\")\n",
        "plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hmm08RYOUr9U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def download_history():\n",
        "  import pickle\n",
        "  from google.colab import files\n",
        "\n",
        "  with open('history_augmented.pkl', 'wb') as f:\n",
        "    pickle.dump(history.history, f)\n",
        "\n",
        "  files.download('history_augmented.pkl')\n",
        "\n",
        "download_history()"
      ],
      "metadata": {
        "id": "GQ3lugTJUwwl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}